{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a04486-8a17-4b12-a0e3-cefc18e51a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Start creating notebook, convert to a .py notebook (the ipynb are default and are stupid)\n",
    "# TODO: We need an external volume with metadata files or we ingest the metadata from the pdfs, that might be easier.\n",
    "# TODO: GRant access to the files we have for test to Brennan (I may have already, check)\n",
    "# TODO: Ingest the metadata to bronze (use autoloader)\n",
    "# TODO: Stream the appended records in bronze to silver, and on the way we're going to use forEachBatch to fetch a pdf, open it, count the pages, and optionally (maybe a different routine) trim to a certain number of pages. Keep it simple. Write the files to a volume OR to the silver delta table (we can think about the best way).\n",
    "# TODO: For agentbricks, figure out if you can grab a binary column from a delta table or have to go to avolume.\n",
    "# TODO: Paramaterize for catalog, schema\n",
    "# TODO: hardcode table and volume names\n",
    "# dbxmetagen.default.pfizer_files_del\n",
    "# TODO: look at the options for reading pdfs and profiling them. What's out there, we want to count pages, and optionally trim. So ,what tools should we use for that, if any?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b764ef4e-6e0a-49fe-a9a3-6cecd19eea0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dacab139-d9b7-4a82-b390-06eeae2888a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"dbxmetagen\")\n",
    "dbutils.widgets.text(\"source_schema\", \"default\")\n",
    "dbutils.widgets.text(\"dest_schema\", \"trimmed_pdfs\")\n",
    "dbutils.widgets.text(\"source_volume\", \"eln_pdfs\")\n",
    "dbutils.widgets.text(\"dest_volume\", \"trimmed_pdfs\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "source_schema = dbutils.widgets.get(\"source_schema\")\n",
    "dest_schema = dbutils.widgets.get(\"dest_schema\")\n",
    "source_volume = dbutils.widgets.get(\"source_volume\")\n",
    "dest_volume = dbutils.widgets.get(\"dest_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33329d00-960c-4d37-afe8-2f0eae478f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pypdf_trim_batch_pdfs(batch_df, batch_id, max_pages: int = 10):\n",
    "    paths = batch_df.select(\"path\").toPandas()[\"path\"]\n",
    "    dest_dir = f\"/dbfs/Volumes/{catalog}/{dest_schema}/{dest_volume}\"\n",
    "\n",
    "    for pdf_path in paths:\n",
    "        local_path = pdf_path.replace(\"dbfs:\", \"\")\n",
    "        basename = os.path.basename(pdf_path)\n",
    "\n",
    "        reader = PdfReader(local_path)\n",
    "        writer = PdfWriter()\n",
    "        for i in range(min(max_pages, len(reader.pages))):\n",
    "            writer.add_page(reader.pages[i])\n",
    "\n",
    "        output_path = os.path.join(dest_dir, basename)\n",
    "        writer.write(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143793d5-846e-414d-b5bc-4b043826c5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bronze\n",
    "df = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "        .load(f\"/Volumes/{catalog}/{source_schema}/{source_volume}\")\n",
    "        .select(\"path\")\n",
    ")\n",
    "\n",
    "# Silver\n",
    "df.writeStream\n",
    "    .foreachBatch(pypdf_trim_batch_pdfs)\n",
    "    .option(\"checkpointLocation\", f\"/Volumes/{catalog}/{dest_schema}/_checkpoints/{dest_volume}\")\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PDF Profiler",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
